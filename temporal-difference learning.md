# temporal-difference learning
From [[reinforcement learning]]
$\physics$
Such methods use bootstrap similarly to [[dynamic programming methods]], and they use experience instead of model as [[Monte Carlo methods]].

- [[TD and MC give different predictions]]
- [[one-step TD prediction]]
- [[SARSA]]
- [[Q-learning]]
- [[expected SARSA]]
- [[maximization bias]]
- [[double Q-learning]]
- [[dueling Q-network]]
- [[n-step TD prediction]]
- [[n-step SARSA]]
- [[n-step off-policy learning with importance sampling]]
- [[n-step tree backup]]
- [[n-step Q(sigma)]])