# Bayesianism vs frequentism
From [[Bayesian inference]]
$\physics$
1. The p-value does not say the probability that the correct answer lies in our interval, but the probability that it lies there if the null-hypothesis is true. Also, this probability is for this test in general, not for that piece of data that we have (because we would need a prior for this, presumably). That is, if we do $N \to \infty$ experiments and choose from them those where the observed variables coincide with the ones we see, it is not certain that among them (even in the limit) $(1 - p)$ part will have answers in the predicted interval. See more [in this article by Janes](https://bayes.wustl.edu/etj/articles/confidence.pdf).
2. Suppose we have a nontrivial graphical model and we want to calculate the probability that one of the variables in it has a certain value. To exclude maximum likelihood approach, suppose that some variables are obviously asymmetric (e.g., X = "the person has cancer"), and there isn't enough data to just ignore that. It seems that in this case insisting on maximum likelihood (and what other non-Bayesian approaches might work here I don't know) would be like insisting on no action in the trolley problem - it won't get rid of inaccurate predictions, even though you have removed all subjectivity.
3. If we want to consider only plausibility, assuming the prior is uniform, what do we do at https://en.wikipedia.org/wiki/Bertrand_paradox_(probability)? That is, let the position of the chord be the parameter of interest, and we know the likelihood function, but our estimate of the parameter depends on how to parameterize the solution space.
4. Two answers are given at https://en.wikipedia.org/wiki/German_tank_problem. I don't really understand which priors they calculated the Bayesian one for, but that's not the point. What does the difference in answers mean in this case, that one method doesn't work, or that many equally correct answers are possible? If there is one answer, is it true that the Frequentist one given there is it (the most optimal one, e.g., with the lowest variance)? Can we even talk about this optimality until we have fixed priors for parameters (or at least priors for parameter parameters, and so on)?
5. [[Copernican principle]]

[[On arithmetic inference]]