# attention
From [[deep learning]] and [[nlp]]

Attention is a powerful mechanism which resembles support vector machines.
Usually, there is a sequence of keys $k_{n} \in \mathbb{R}^{d_{in}}$, a sequence of values $v_{n} \in \mathbb{R}^{d_{out}}$ and a query $q \in \mathbb{R}^{d_{in}}$. Also we need a function $S: \mathbb{R}^{d_{in}} \times \mathbb{R}^{d_{in}} \to \mathbb{R}$ that will calculate similarities between the keys and queries. Then the attention is defined as:
$$\mathrm{Attn}(K, V, q) = \sum\limits_{i=1}^{n} S(k_{i}, q) v_{i} = \langle S(K, q), V \rangle$$
An example of such attention is [[dot product attention]].

The definition is not rigid. We can add some nonlinearities, as in [[scaled softmax attention]] or [cosFormer](https://arxiv.org/abs/2202.08791). Also, keys and queries don't have to be of same dimension or even to be real vectors, as long as $S$ can work with them.


## Multi-head attention
Here we have some $d$ that is the dimension of inputs and outputs of this layer and $h$ which is the number of heads.
$$ Q \in \mathbb{R}^{n \times d}, K, V \in \mathbb{R}^{m \times d}$$
$$ W_{i}^{Q} \in \mathbb{R}^{d \times d_{q}}, W_{i}^{K}\in \mathbb{R}^{d \times d_{k}}, W_{i}^{V} \in \mathbb{R}^{d \times d_{v}}, W^{O}\in \mathbb{R}^{hd_{v}\times d}$$
$$ \mathrm{head}_{i}(K, V, Q) = \mathrm{Attention}(KW_{i}^{K}, VW_{i}^{V}, QW_{i}^{Q})$$
$$ \mathrm{MultiHead}(K, V, Q) = \mathrm{concat}(\mathrm{head}_{1}, \dots, \mathrm{head}_{h}) W^{O}$$

## Self-attention and sources of inputs
In the diagrams of architectures attention layers usually get one matrix and produce one matrix, but as we see, the formula requires three of them. Usually it means that key and value tensors are the same, and in case of self-attention the query tensors are also the same. It’s actually the definition of self-attention: when we take both queries and key-values from the same place. If there are decoder and encoder (as in [[seq2seq]]), usually there is also an attention layer that takes queries from the output of previous decoder layer and key-values from the output of the last encoder layer. 

## Why attention?
The main benefit of attention layers is that information can flow from any position to any other in just one layer. To achieve the same in convolutional models we would have to set kernel size equal to $n$, which will give the same running time, and also it won’t generalize to other input lengths. And it’s better than recurrent networks, because we can massively parallelize the computations inside attention layer, while in [[RNN]] we must execute them consecutively.

## Attention in [[seq2seq]]
When the decoder needs to predict the next token, first some coefficients are computed for each of the hidden states that were generated by encoder while reading the input sequence. To compute each of them we use attention function that takes the hidden state of encoder that we are considering now and the last known hidden state of decoder. Then we just combine these encoder hidden states linearly with the calculated coefficients. It might lose some information from them, but it’s still much better than just looking at the last hidden state. This linear combination of encoder hidden states is called *context vector*.

## See also
- [[not all attention is needed; gated attention network for sequence data]]
- [[squeeze-and-excitation block]]
- [[retro]]

## Random thoughts
maybe attention can be used for the model weights, so that at each batch the right set of weights will be updated. maybe it will allow faster convergence (like with adaptive learning rate) or less forgetting in multitasking. maybe this can be done during the test time to increase accuracy, though it’s similar to gated activations in [[gated pixelcnn]].

maybe attention can depend on some hidden state.